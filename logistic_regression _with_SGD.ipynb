{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# How To Implement Logistic Regression With Stochastic Gradient Descent From Scratch With Python\n",
    "**파이썬을 이용해 SGD를 이용한 로지스틱 회귀분석 실행하기**\n",
    "\n",
    "[quotation](https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression\n",
    " \n",
    "로지스틱 회귀분석과 선형회귀분석의 결정적 차이점은 로지스틱 회귀분석의 결과값이 **이분점** 이라는 점\n",
    "\n",
    "\n",
    "**Input values (X)** : 가중치(weight) or 계수(coefficient)와 함께 쓰임 \n",
    "\n",
    "**Output values (Y)** : 0 or 1\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = exp(b_0 + b_1 \\ast x_1)/(1 + exp(b_0 + b_1 \\ast x_1))\n",
    "\\end{align}\n",
    "\n",
    "                                                                  단순화\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = 1.0/ (1.0 + exp(-(b0 + b1 \\ast x1)))\n",
    "\\end{align}\n",
    "\n",
    "**yhat** : 예측결과 (= 예측확률)\n",
    "\n",
    "yhat 예측은 0과 1 사이의 실제 값이며, 정수 값으로 반올림하고 예측된 클래스 값으로 매핑해야 한다.\n",
    "\n",
    "**b0** : 편차 or 기울기\n",
    "\n",
    "**b1** : x1에 대한 계수 \n",
    "\n",
    "* 입력 데이터의 각각의 열은 트레이닝 데이터를 통해 얻어진 b 계수와 연관된다.  \n",
    "* 메모리 또는 파일에 저장할 모델의 실제 표현은 방정식의 계수다. \n",
    "* 로지스틱 회귀분석 알고리즘의 계수는 나의 트레이닝 데이터를 통해 추정해야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stochastic Gradient Decent (= SGD)\n",
    "**여기에는 주어진 지점에서의 미분뿐만 아니라 손실의 형태도 알아야 구배를 알고 그 방향으로 움직일 수 있다 (예: 최소값을 향한 downhill).**\n".
    "\n",
    "**Gradient Descent는 손실 함수의 구배를 따라 함수를 최소화하는 과정이다.**\n",
    "\n",
    "* 손실과 미분을 알아야 주어진 지점에서 구배를 알고 그 방향으로 움직일 수 있다.\n",
    "  e.g) dowmhill towards the minimum value .\n",
    "\n",
    "* 머신러닝에서 SGD의 반복마다 계수를 평가하고 업데이트 하는 기법을 사용하여 훈련 데이터에 대한 모델의 오류를 최소화한다.\n",
    "\n",
    "* 최적화 알고리즘이 작동하는 방법은 각 교육사례를 한 번에 하나씩 표시하는것. \n",
    "  모델은 훈련 사례를 예측하고, 다음 예측에 대한 오류를 줄이를 줄이기 위해 오류를 계산하고 모델을 업데이트한다.\n",
    "\n",
    "* 이 절차는 훈련 데이터에서 모델에 대한 최소 오차를 나타내는 모델의 계수 집합을 찾는데 사용한다.\n",
    "  각 반복에서 기계학습 언어의 계수 (b)는 아래의 방정식을 통해 업데이트 된다.\n",
    "\n",
    "\\begin{align}\n",
    "b = b + learning \\_ rate \\ast (y-\\hat{y}) \\ast \\hat{y} \\ast (1 - \\hat{y}) \\ast x \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**b** : 최적화 과정의 가중치 또는 계수\n",
    "\n",
    "**learning_rate** : 임의로 설정 해줘야함 (e.g. 0.01)\n",
    "\n",
    "**yhat** : 계수와 입력변수 x를 통해 예측된 예측값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pima Indians Diabetes Dataset\n",
    "**분석에 사용할 데이터셋**\n",
    "\n",
    "[원데이터](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)\n",
    "\n",
    "Pima Idian의 기본적인 의학적 세부사항을 통해 당뇨병의 발생을 5년 이내에 예측하는것 \n",
    "\n",
    "당뇨병 발생 여부 : 0, 1\n",
    "\n",
    "0 : 미발생, 1 :발생   \n",
    "\n",
    "다운로드한 데이터셋을 'pima-indians-diabetes.csv' 로 저장할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Making Prediction\n",
    "**계수가 주어졌을때 예측함수**\n",
    "\n",
    "```python\n",
    "def predict(row, coefficients):\n",
    "        yhat = coefficients[0]\n",
    "\t    for i in range(len(row)-1):\n",
    "\t\t    yhat += coefficients[i + 1] * row[i]\n",
    "\t    return 1.0 / (1.0 + exp(-yhat))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0.000, Predicted=0.299 [0]\n",
      "Expected=0.000, Predicted=0.146 [0]\n",
      "Expected=0.000, Predicted=0.085 [0]\n",
      "Expected=0.000, Predicted=0.220 [0]\n",
      "Expected=0.000, Predicted=0.247 [0]\n",
      "Expected=1.000, Predicted=0.955 [1]\n",
      "Expected=1.000, Predicted=0.862 [1]\n",
      "Expected=1.000, Predicted=0.972 [1]\n",
      "Expected=1.000, Predicted=0.999 [1]\n",
      "Expected=1.000, Predicted=0.905 [1]\n"
     ]
    }
   ],
   "source": [
    "# 작은 데이터 셋을 통해 predict() 함수 테스트\n",
    "from math import exp\n",
    "\n",
    "# 계수를 통한 예측 함수\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))\n",
    "\n",
    "# 예측 테스트\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "\n",
    "coef = [-0.406605464, 0.852573316, -1.104746259]\n",
    "for row in dataset:\n",
    "\tyhat = predict(row, coef)\n",
    "\tprint(\"Expected=%.3f, Predicted=%.3f [%d]\" % (row[-1], yhat, round(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Estimating Coefficients\n",
    "\n",
    "**Learning Rate** : 각 계수가 업데이트 될 때마다 수정되는 양을 제한하는데 사용\n",
    "\n",
    "**Epochs** : 계수를 업데이트 하는 동안 교육 데이터를 실행하는 횟수\n",
    "\n",
    "\n",
    "* 3가지의 루틴이 함수 안에서 실현되야함.\n",
    "    1. 각각의 epcoh 마다 반복\n",
    "    2. 각각의 교육데이터의 각각의 행마다 반복\n",
    "    3. 하나의 epoch를 실행할때 하나의 행을 통해 계수 업데이트\n",
    "\n",
    "\n",
    "* 계수(coefficient) : 만들어진 오차 모델을 기반으로 업데이트\n",
    "\n",
    "\n",
    "* 오차(error) : 예상 출력 값과 후보 계수를 사용한 예측값의 차이로 계산\n",
    "\n",
    "\n",
    "* 각 입력 속성에 가중치를 부여하는 계수가 하나 있으며, 이러한 계수는 다음과 같이 일관된 방식으로 업데이트된다.\n",
    "\n",
    " **b1**\n",
    "\n",
    "\\begin{align}\n",
    "b_1(t+1) = b_1(t)+learning\\_rate \\ast (y(t)-\\hat{y}(t))\\ast \\hat{y}(t) \\ast (1- \\hat{y}(t)) \\ast x_1(t)\n",
    "\\end{align}\n",
    "\n",
    "* 절편으로도 불리는 목록의 앞에 있는 특수계수는 특정 입력값과 연계되지 않았기 때문에 입력값 없는것을 제외하고 유사한 방법으로 업데이트 된다.\n",
    "\n",
    " **b0**\n",
    "\n",
    "\\begin{align}\n",
    "b_0(t+1) = b_0(t)+learning\\_rate \\ast (y(t)-\\hat{y}(t)) \\ast \\hat{y}(t) \\ast (1-\\hat{y}(t))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**SGD를 이용한 로지스틱 회귀분석 계수 예측 함수**\n",
    "```python\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tfor row in train:\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "\t\t\terror = row[-1] - yhat\n",
    "\t\t\tsum_error += error**2\n",
    "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn coef\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.300, error=2.217\n",
      ">epoch=1, lrate=0.300, error=1.613\n",
      ">epoch=2, lrate=0.300, error=1.113\n",
      ">epoch=3, lrate=0.300, error=0.827\n",
      ">epoch=4, lrate=0.300, error=0.623\n",
      ">epoch=5, lrate=0.300, error=0.494\n",
      ">epoch=6, lrate=0.300, error=0.412\n",
      ">epoch=7, lrate=0.300, error=0.354\n",
      ">epoch=8, lrate=0.300, error=0.310\n",
      ">epoch=9, lrate=0.300, error=0.276\n",
      ">epoch=10, lrate=0.300, error=0.248\n",
      ">epoch=11, lrate=0.300, error=0.224\n",
      ">epoch=12, lrate=0.300, error=0.205\n",
      ">epoch=13, lrate=0.300, error=0.189\n",
      ">epoch=14, lrate=0.300, error=0.174\n",
      ">epoch=15, lrate=0.300, error=0.162\n",
      ">epoch=16, lrate=0.300, error=0.151\n",
      ">epoch=17, lrate=0.300, error=0.142\n",
      ">epoch=18, lrate=0.300, error=0.134\n",
      ">epoch=19, lrate=0.300, error=0.126\n",
      ">epoch=20, lrate=0.300, error=0.119\n",
      ">epoch=21, lrate=0.300, error=0.113\n",
      ">epoch=22, lrate=0.300, error=0.108\n",
      ">epoch=23, lrate=0.300, error=0.103\n",
      ">epoch=24, lrate=0.300, error=0.098\n",
      ">epoch=25, lrate=0.300, error=0.094\n",
      ">epoch=26, lrate=0.300, error=0.090\n",
      ">epoch=27, lrate=0.300, error=0.087\n",
      ">epoch=28, lrate=0.300, error=0.084\n",
      ">epoch=29, lrate=0.300, error=0.080\n",
      ">epoch=30, lrate=0.300, error=0.078\n",
      ">epoch=31, lrate=0.300, error=0.075\n",
      ">epoch=32, lrate=0.300, error=0.073\n",
      ">epoch=33, lrate=0.300, error=0.070\n",
      ">epoch=34, lrate=0.300, error=0.068\n",
      ">epoch=35, lrate=0.300, error=0.066\n",
      ">epoch=36, lrate=0.300, error=0.064\n",
      ">epoch=37, lrate=0.300, error=0.062\n",
      ">epoch=38, lrate=0.300, error=0.060\n",
      ">epoch=39, lrate=0.300, error=0.059\n",
      ">epoch=40, lrate=0.300, error=0.057\n",
      ">epoch=41, lrate=0.300, error=0.056\n",
      ">epoch=42, lrate=0.300, error=0.054\n",
      ">epoch=43, lrate=0.300, error=0.053\n",
      ">epoch=44, lrate=0.300, error=0.052\n",
      ">epoch=45, lrate=0.300, error=0.051\n",
      ">epoch=46, lrate=0.300, error=0.050\n",
      ">epoch=47, lrate=0.300, error=0.048\n",
      ">epoch=48, lrate=0.300, error=0.047\n",
      ">epoch=49, lrate=0.300, error=0.046\n",
      "[-0.7282458823836234, 1.3156310025113396, -1.8978207115152121]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 작은 데이터셋을 통해 coefficients_sgd() 함수 테스트\n",
    "from math import exp\n",
    " \n",
    "# 계수를 통한 예측 함수 \n",
    "def predict(row, coefficients):\n",
    "\tyhat = coefficients[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += coefficients[i + 1] * row[i]\n",
    "\treturn 1.0 / (1.0 + exp(-yhat))\n",
    " \n",
    "# SGD를 이용한 로지스틱 회귀분석 계수 예측 함수\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]  # 컴프리핸션 \n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tfor row in train:\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "\t\t\terror = row[-1] - yhat  # error : y - yhat\n",
    "\t\t\tsum_error += error**2  # sum_error : SSE\n",
    "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn coef\n",
    " \n",
    "# 계수 계산\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "l_rate = 0.3  # l_rate : 임의의 learning rate \n",
    "n_epoch = 50  # n_epoch : 교육데이터를 50번 실행함\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Diabetes Prediction\n",
    "**이번에는 당뇨병 데이터셋을 SGD를 사용한 로지스틱 회귀모델에 교육시킨다. **\n",
    "\n",
    "* 데이터셋을 받았을때 \n",
    "    1. numeric 형태로 바꿔줘야하고 \n",
    "    2. 각각의 열은 0에서 1 범위로 정규화 된다. \n",
    "       \n",
    "       * numeric형태 : load_csv(), str_column_float() 함수 사용\n",
    "        * 정규화 : data_minmax(), normalize_dataset() 함수 사용\n",
    "\n",
    "\n",
    "* k-폴드 교차검증을 사용, 보이지 않는 데이터에 대한 학습된 모델의 성능을 추정\n",
    "    * k 모델을 구성 및 평가하고 평균 모델 성능으로 성능을 추정한다는 것을 의미\n",
    "    * 분류의 정확도를 사용하여 각 모델을 평가\n",
    "        \n",
    "        * cross_validation_split(), accuracy_metric(), evaluate_algorithm() 함수 사용\n",
    "\n",
    "## k-fold cross validation 란?\n",
    "** 모델 평가를 위해 데이터를 훈련세트와 검증 세트로 나눌 때 편향을 방지하기 위해 사용** \n",
    "\n",
    "* 적은 데이터에서 테스트와 트레이닝 데이터를 나눠 학습\n",
    "    * 그 테스트에는 효과적인 예측결과를 가져올 수 있지만\n",
    "    * 실제 새로 업데이트되는 데이터에서의 예측력은 보장할 수 없다.\n",
    "  \n",
    "\n",
    "데이터를 K개로 나누어 K-1개를 분할하고 나머지는 평가에 사용\n",
    "\n",
    "모델의 검증 점수는 **K개의 검증 점수 평균**이 된다.\n",
    "\n",
    "ex) 5개의 그룹 중 데이터 1을 테스트 데이터, 나머지 데이터들을 트레이닝 데이터로 두고 학습시킨 뒤, 평가하고 그 값을 기록\n",
    "    다음은 데이터 2 를 테스트 데이터, 나머지 트레이닝 데이터로 두고 학습 \n",
    "    1 ~ 5 까지 반복\n",
    "\n",
    "**이 방법을 특정 람다 값에서 실행해본 후 가장 좋은 평균값을 보여주는 람다 선택**\n",
    "\n",
    "*주의사항*\n",
    "\n",
    "* 대표성 있는 데이터 : 0 ~ 9 데이터를 훈련 시킬 때 0 ~ 7은 훈련, 8 ~ 9는 테스트로 사용가능, \n",
    "    **데이터를 나누기 전에 썪어줌으로써 대표성을 가지게 해야함**\n",
    "* 시간의 방향 : 과거로부터 미래를 예측,\n",
    "     **데이터를 섞어서는 안됨.**\n",
    "* 데이터 중복 : 한 데이터가 두 번 등장하면 신뢰도가 크게 떨어짐,\n",
    "    **훈련세트와 검증 세트가 중복되지 않는지 확인**\n",
    "\n",
    "[보충학습](https://blog.naver.com/dnjswns2280/221532535858)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [73.8562091503268, 78.43137254901961, 81.69934640522875, 75.81699346405229, 75.81699346405229]\n",
      "Mean Accuracy: 77.124%\n"
     ]
    }
   ],
   "source": [
    "# 당뇨병 데이터에 대한 로지스틱 회귀분석\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import exp\n",
    " \n",
    "# CSV파일을 불러온다. \n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# 열을 실수로 변환한다. \n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# 각각의 열의 최소와 최대를 구해줌\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_min = min(col_values)\n",
    "\t\tvalue_max = max(col_values)\n",
    "\t\tminmax.append([value_min, value_max])\n",
    "\treturn minmax\n",
    " \n",
    "# 데이타셋의 열을 0-1 범위로 축약해 준다.\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    "# 데이터셋을 K-fold 교차검증을 통해 나누어 준다.\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# 정확도 백분율을 계산한다. \n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# 교차 검증 분할을 사용하여 알고리즘 구성\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "# 계수 예측값\n",
    "def predict(row, coefficients):\n",
    "\tyhat = coefficients[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += coefficients[i + 1] * row[i]\n",
    "\treturn 1.0 / (1.0 + exp(-yhat))\n",
    " \n",
    "# SGD를 활용한 로지스틱 회귀분석 계수 추정\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "\t\t\terror = row[-1] - yhat\n",
    "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "\treturn coef\n",
    " \n",
    "# SGD를 활용한 선형 회귀분석 알고리즘 \n",
    "def logistic_regression(train, test, l_rate, n_epoch):\n",
    "\tpredictions = list()\n",
    "\tcoef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "\tfor row in test:\n",
    "\t\tyhat = predict(row, coef)\n",
    "\t\tyhat = round(yhat)\n",
    "\t\tpredictions.append(yhat)\n",
    "\treturn(predictions)\n",
    " \n",
    "# 당뇨병 데이터셋을 통한logistic regression 알고리즘 테스트\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# 일반화\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# 알고리즘 평가\n",
    "n_folds = 5\n",
    "l_rate = 0.1\n",
    "n_epoch = 100\n",
    "scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**k** : k-fold cross validation에서의 k = 5\n",
    "\n",
    "약간의 시행을 통해 **learning_rate** : 0.1, **epochs** : 100 을 선택 \n",
    "\n",
    "**learning_rate**는 임으로 정해줄 수 있다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
